# Vynel

**Private by default. Local by design.**

Vynel is a fully client-side AI chat application that runs **entirely in your browser** using **WebGPU acceleration**.  
All inference happens locally on your machine â€” **no backend server, no cloud AI, no API keys**.

> âš ï¸ **Scope notice**  
> Vynel is intentionally **GPU-only**.  
> CPU / WASM fallback has been removed to keep the architecture clean and correct.

---

## âœ¨ Features

- ğŸ”’ **100% local inference** (no backend, no API keys)
- âš¡ **WebGPU acceleration** (MLC / WebLLM)
- ğŸ§  **Multiple large language models**
- ğŸ’¬ **Streaming responses** (ChatGPT-style UX)
- ğŸ”„ **Model selector with persistence**
- ğŸ–¤ **Dark UI** (black & orange theme)

---

## ğŸ§  Supported Models (WebGPU only)

All models are compiled with **MLC** and require **WebGPU**.

- **TinyLlama 1.1B** â€” Fast
- **Llama 3 8B** â€” Smart
- **Mistral 7B** â€” Deep
- **Gemma 2 2B** â€” Balanced

> All models run locally on your GPU.  
> âŒ No model runs on CPU at this stage.

---

## ğŸ“‹ System Requirements

### âœ… Operating System

- **Windows 10 / 11 only**
- macOS and Linux are **not supported yet**

---

### âœ… Browser

- **Google Chrome (required)**
- Other Chromium-based browsers *may* work, but only Chrome is supported

---

### âœ… Hardware

- GPU with **WebGPU support**
- Modern **Intel / AMD / NVIDIA** GPU  
  (integrated or discrete)

---

## ğŸ§ª Chrome Setup (VERY IMPORTANT)

WebGPU **must** be fully enabled in Chrome.

---

### Step 1: Enable WebGPU Flags

1. Open **Google Chrome**
2. Navigate to:
3. Enable:
- **WebGPU**
- **Unsafe WebGPU Support** (if available)
4. Relaunch Chrome

---

### Step 2: Enable Hardware Acceleration

1. Open **Chrome Settings**
2. Go to **System**
3. Enable:
- âœ… *Use hardware acceleration when available*
4. Relaunch Chrome

---




## Installation (from scratch)

These steps assume nothing is installed on the system.

## Step 1: Install Node.js

Install Node.js 20+
ğŸ‘‰ https://nodejs.org/

Verify installation:

node -v
npm -v

## Step 2: Clone the Repository
git clone https://github.com/<your-username>/vynel.git
cd vynel

## Step 3: Install Dependencies
npm install

Installs:
React
Vite
@mlc-ai/web-llm
react-markdown
remark-gfm

## Step 4: Start the Development Server
npm run dev

##You should see:

Local: http://localhost:5173/


## Open the URL in Google Chrome.


## ğŸš€ Usage

Open Vynel in Chrome

Select a model from the dropdown

Start chatting

Responses stream in real time

Use Stop to interrupt generation

Use Clear chat to reset context

âš ï¸ If WebGPU is unavailable, the app will fail to load models.


Project Structure
src/
â”œâ”€ components/
â”‚  â”œâ”€ ChatApp.jsx      # Main UI + logic
â”‚  â”œâ”€ ChatInput.jsx   # Input + send/stop
â”‚  â””â”€ Header.jsx      # App header
â”‚
â”œâ”€ inference/
â”‚  â”œâ”€ index.js        # WebGPU inference manager
â”‚  â””â”€ webllm.js       # WebLLM / MLC engine
â”‚
â”œâ”€ main.jsx
â”œâ”€ App.jsx

ğŸ” Privacy

No backend

No cloud inference

No API keys

No telemetry

Model files are downloaded locally

Everything runs entirely on your machine.

â­ Final Note

Vynel exists to prove that serious, private, local AI in the browser is already possible.

If you like this project, consider starring the repo â­